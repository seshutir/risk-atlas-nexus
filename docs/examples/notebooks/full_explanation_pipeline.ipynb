{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a92b30a6-4a3c-4215-a0b8-3f61480ccba3",
   "metadata": {},
   "source": [
    "# Generating global explanations of LLM-as-a-Judge using GloVE algorithm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e8712c-f06a-4bad-970b-fdcf85c3aa4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.risk_atlas_nexus import RiskAtlasNexus\n",
    "from risk_policy_distillation.datasets.prompt_response_dataset import (\n",
    "    PromptResponseDataset,\n",
    ")\n",
    "from risk_atlas_nexus.blocks.inference import RITSInferenceEngine\n",
    "from risk_atlas_nexus.blocks.inference.params import RITSInferenceEngineParams\n",
    "\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2f2b11f-d205-4f81-93bb-9ab3dd785db8",
   "metadata": {},
   "source": [
    "#### Create a dataset\n",
    "\n",
    "To explain the LLM-as-a-Judge you need to provide a dataset. [AbstractDataset](../src/models/datasets/abs_dataset.py) class provides a wrapper for a dataframe you want to explain. You can use [PromptDataset](../src/models/datasets/prompt_dataset.py) or [PromptResponseDataset](../src/models/datasets/prompt_response_dataset.py) depending on whether your dataframe consists of only prompts or prompt-response pairs. You can also create a custom dataset by inheriting the Dataset class.\n",
    "\n",
    "You have to provide a config with information on column name mapping. Additional parameters: _flip_labels_ indicates whether labels of the dataframe should be flipped in preprocessing step (e.g. for BeaverTails where labels indicate that the content is safe rather than harmful); _split_ indicates whether a train-val-test split needs to be performed during preprocessing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7bc4d137-23e8-47da-afbd-88947bc07643",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# get the dataset\n",
    "ds = load_dataset(\"PKU-Alignment/BeaverTails\")\n",
    "dataframe = ds.data[\"330k_train\"].table.to_pandas()\n",
    "\n",
    "# sample for testing purposes\n",
    "dataframe = dataframe.sample(10)\n",
    "\n",
    "# provide some information in the data config\n",
    "data_config = {\n",
    "    \"dataset_name\": \"BeaverTails\",\n",
    "    \"index_col\": \"\",  # if there is no index column it will be automatically populated\n",
    "    \"prompt_col\": \"prompt\",\n",
    "    \"response_col\": \"response\",\n",
    "    \"label_col\": \"is_safe\",\n",
    "}\n",
    "\n",
    "dataset = PromptResponseDataset(dataframe=dataframe, config=data_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c952984",
   "metadata": {},
   "source": [
    "### Create a guardian config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ef0055-4e03-4ed5-9806-b3fc3609cd80",
   "metadata": {},
   "outputs": [],
   "source": [
    "guardian_config = {\n",
    "    \"task\": f\"Harm detection\",\n",
    "    \"criterion\": f\"Harm\",\n",
    "    \"criterion_definition\": f\"The message has harmful by common-sense.\",\n",
    "    \"labels\": [0, 1],\n",
    "    \"label_names\": [\"harmless\", \"harmful\"],\n",
    "    \"output_labels\": [\"no\", \"yes\"],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1db0aa6a",
   "metadata": {},
   "source": [
    "##### Policy Distillation uses granite gurdian as a LLM-as-a-Judge, and other Large Language Models (LLMs) to query at various stages in the pipeline. Therefore requires access to inference or call the model.\n",
    "\n",
    "**Available Inference Engines**: WML, Ollama, vLLM, RITS. Please follow the [Inference APIs](https://github.com/IBM/risk-atlas-nexus?tab=readme-ov-file#install-for-inference-apis) guide before going ahead.\n",
    "\n",
    "_Note:_ RITS is intended solely for internal IBM use and requires TUNNELALL VPN for access.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa7e9fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "guardian_judge = RITSInferenceEngine(\n",
    "    model_name_or_path=\"ibm-granite/granite-guardian-3.3-8b\",\n",
    "    credentials={\n",
    "        \"api_key\": \"cbc683b3a1a7c52d2a73008b785d2811\",\n",
    "        \"api_url\": \"https://inference-3scale-apicast-production.apps.rits.fmaas.res.ibm.com\",\n",
    "    },\n",
    "    parameters=RITSInferenceEngineParams(\n",
    "        logprobs=True, top_logprobs=10, temperature=0.0\n",
    "    ),\n",
    ")\n",
    "\n",
    "llm_component = RITSInferenceEngine(\n",
    "    model_name_or_path=\"meta-llama/llama-3-3-70b-instruct\",\n",
    "    credentials={\n",
    "        \"api_key\": \"cbc683b3a1a7c52d2a73008b785d2811\",\n",
    "        \"api_url\": \"https://inference-3scale-apicast-production.apps.rits.fmaas.res.ibm.com\",\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d419d3d-922e-4e10-aefe-3e8617e65e01",
   "metadata": {},
   "source": [
    "### Create and run the explanation generation pipeline\n",
    "\n",
    "Pipeline streamlines local and global explanation generation process. Extractor executes the CLoVE algorithm and generates a set of local explanations, and Clusterer executes GloVE algorithm and merges the local explanations into a global one.\n",
    "\n",
    "Pass `lime=False` to pipeline creation step if no local word-based verification is done. SImilarly, use `fr=False` if FactReasoner is not used to verify global explanations.\n",
    "\n",
    "The resulting local and global explanations are saved in the path folder passed to the pipeline.run() call.\n",
    "The execution logs can be found in the logs folder.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee994e0-9e5d-4903-81af-22c78c240fb3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-10-28 10:04:01:384] - INFO - RiskAtlasNexus - Created RiskAtlasNexus instance. Base_dir: None\n",
      "[2025-10-28 10:04:01:912] - INFO - RiskAtlasNexus - Created RITS inference engine.\n",
      "[2025-10-28 10:04:02:269] - INFO - RiskAtlasNexus - Created RITS inference engine.\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e52f1d3e33d644f4b106b07225d11a03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inferring with RITS:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50178fce8ec648428793e76f1a50c535",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inferring with RITS:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c5723aa2ec0440192bed296e0d98d79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inferring with RITS:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ffbb69ad915e4f8086725f1203387edc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inferring with RITS:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "062a6475db1a400eb243aa473d32d5df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inferring with RITS:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a013882e1fd74cf0a4866e128ecaab22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inferring with RITS:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "163be97f899c45d0938cde6b8a69e4c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inferring with RITS:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50949a8261d444798e6609fe80e859ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inferring with RITS:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2aaf184c0ce546ee87dc17ff6cad364a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inferring with RITS:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5fb28870f2e64817a0d59d199f3de1a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inferring with RITS:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19c097055920477ca41595682b7b5a9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inferring with RITS:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:22, 22.96s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe8f0ee750ec41918c9bdc9290831068",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inferring with RITS:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b0fd14394354685b6c01e5292f5c305",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inferring with RITS:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a795946ad1f84ac1b5b8ee317e644a31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inferring with RITS:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1811d4fce4744ae9dceeda1d1f6b130",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inferring with RITS:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a389ad084d5445539a794ed49b7305da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inferring with RITS:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a943978410147289b8bb5755118b32b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inferring with RITS:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d53c9e0512846008427ea77a37d3738",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inferring with RITS:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abaefc2adb5e453897c2a76ba3f86e9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inferring with RITS:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0913f72fe4b548bebcdc976acee68f58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inferring with RITS:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c9b2e4314004c9ebf62d177c4967ecf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inferring with RITS:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68b75974e1c142df8c64c0f5cd95f7ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inferring with RITS:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5bcfbb841924f4c8c52d839b50a679e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inferring with RITS:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [00:53, 27.28s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b489ba14d7a3438db3ee6044cc2bc372",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inferring with RITS:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d973019286a4e63ae3cff2c9225e4bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inferring with RITS:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52be093eff544242b9271b175c1c6b32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inferring with RITS:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c602c71624a4cf091172a4dcc0e15fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inferring with RITS:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9db0a384a4b4d21bd19a6ea91a4a34c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inferring with RITS:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "998fad5132e5436796c90cf340d2c85a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inferring with RITS:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8879a1fc3e5a4864a67be067f0a684e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inferring with RITS:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2fb4e16e7a64473bda60341b432c45f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inferring with RITS:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15969907f039415fb87f801cb424857f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inferring with RITS:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e77b3a46b3f546b09f094d6dbac39dc3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inferring with RITS:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e34693c88504dbab4ba1f6c7074ab0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inferring with RITS:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [01:18, 26.50s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48e6c63a95b14283bae56f94a0cd6179",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inferring with RITS:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0a3db2559c0489794ac960f8de80a51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inferring with RITS:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f76d372a0e0449ba01a4b31918a3513",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inferring with RITS:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf1de83f9f7a44a587a056553b368a94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inferring with RITS:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbb096dc5cba4ca7922f3cde5b4a82da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inferring with RITS:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93c90af397934c6c8fd22a779a6f59d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inferring with RITS:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09c13e5e04274be49af01acda6def1cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inferring with RITS:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c007a41c4df349e18dc1ba965d69f1f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inferring with RITS:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83be4480bd8e48dd818100abcf6f09d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inferring with RITS:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b51d08161c84f2e90ffc72cd7a50b27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inferring with RITS:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce85984486644627b3072027fae00146",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inferring with RITS:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f6f4d6009d943dd92c3526d53a8f263",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inferring with RITS:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [01:32, 21.53s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac19626f2d224d9896d80732ba3ed66f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inferring with RITS:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9749c4b8029345bfad6feeb969c037d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inferring with RITS:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea2517cf84e44201b50b9cca05eb85df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inferring with RITS:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16875f5d010a4df8b6a211a42612eaee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inferring with RITS:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb21b27dcb384f0fbf2443216c530037",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inferring with RITS:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81bc503be0e54ae09186e6f57cbf8c9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inferring with RITS:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b83707a9c996453b92e1a6ef2aee2a24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inferring with RITS:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ab28595fb2244418a0bbe161d4aaa29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inferring with RITS:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f48ae5964ee6445488cbdd35a9469bf4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inferring with RITS:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9946ce34158e4878adab6692b304ad6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inferring with RITS:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfee12cd6f834cc4ba7fe2773c1283b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inferring with RITS:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bdd1f16c883a4e33984eaed7bbb67ff9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inferring with RITS:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [02:20, 31.05s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc34ff1324584f55834902e2ccde81c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inferring with RITS:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9662647cd274ca0b6382cbb8386ba04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inferring with RITS:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49770890dfe044739668b86b7e61634a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inferring with RITS:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cdd46bfbfc254a8da264f2cfba7ac654",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inferring with RITS:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3d2d21b408148208ec35fbc9ea2215c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inferring with RITS:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e74f2fb12c8f48e0b11bbac134405a54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inferring with RITS:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1fc98c3efea46ada42f3f8a41f01b72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inferring with RITS:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fc7eb2d91604da0bf3d39be082e4ed6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inferring with RITS:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b39ac7e01c784407be319039017e6fda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inferring with RITS:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c3e9d1ec4b5454a8cdd490a5c4e782e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inferring with RITS:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4724fa8e1ee48449a23050faaf83363",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inferring with RITS:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "995abf2415be427ea51234a703a3d67f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inferring with RITS:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6it [02:35, 25.51s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd11b10ccd2d4dcb9190266513ec033b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inferring with RITS:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5ab95613e1a4deb91b95e403db01d65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inferring with RITS:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec8e0d014e754f2ba64c80c73ce9648c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inferring with RITS:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb0f9ee7d19b44d0b902fc63f2a8a8bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inferring with RITS:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "255a23b73ded4b8791cb9c751272e2c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inferring with RITS:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "838b85158e6c4663a2a6ee22ec2def1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inferring with RITS:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "020af458a16d4574bab6df3b3e1deb97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inferring with RITS:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81fa72f8a8604e8ca0d732953ed78f44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inferring with RITS:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8905a72f8194d1cbc8039690301599e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inferring with RITS:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d114e9bbcf7466da4f4f5c5fef8bb3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inferring with RITS:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "011b72733fcd4ecfbe0a91389e9b3854",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inferring with RITS:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "912e9e86312645e19a81f4566fb40e85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inferring with RITS:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7it [03:39, 38.14s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f5e4a228e8949efbd13d38b1b4abe90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inferring with RITS:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1868ebfff61c446fa7021b80dd96f479",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inferring with RITS:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2da132d4e63d49afb24ecd16ad6dc683",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inferring with RITS:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abb533bfcf4348e38a0a5ae547cd9c72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inferring with RITS:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7060ae2282a446319b81e213d8d75996",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inferring with RITS:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a99b810b5d04834a9f40188ddfa6caf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inferring with RITS:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad5e71c8d87f41138d7a92ed2e010689",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inferring with RITS:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef5fefe36c12459fb4ff12f1a926c137",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inferring with RITS:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26cf4c32740343b0931d5315deb298eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inferring with RITS:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "306a8512a5c04477aff44692fc8feca5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inferring with RITS:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8it [04:10, 35.80s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ed735d6afb84392a8ca428ac9a15857",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inferring with RITS:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8b675df816543cbbe0c2b9d2accc5f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inferring with RITS:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1cba3cd9e9a44c6987eaa67a46fa9202",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inferring with RITS:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e2989419760436fbfb37d56cbf867da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inferring with RITS:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66df3dd32be741f3a80d6f06baef6ac3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inferring with RITS:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23628ca62f054a62a823d518a3b4a1fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inferring with RITS:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a727bda6b4eb45c5a27aeddc6ecb189b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inferring with RITS:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eac3a766223f4b8b82b36ec860ebd50b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inferring with RITS:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e13a42e2df14ba59d3c48733e873a2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inferring with RITS:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34603c2375ef4a9fa16261af84b79615",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inferring with RITS:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38651ac95d0f493991d27654b1c467f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inferring with RITS:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9it [04:35, 32.57s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06b9199857ea4b8db8debd84bdbfd863",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inferring with RITS:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb50f33b089a49c1b905c4de8fafa4c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inferring with RITS:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf0457da96714d07b02d0db7c78f96a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inferring with RITS:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52aa3a67c23047e292d9a38ad4efa27c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inferring with RITS:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "094134f4f1d24cf49f212cf78761b688",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inferring with RITS:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "675dc3f3cae04bd9b3a637fe8c6f4288",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inferring with RITS:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70b5b35e23dc4760a650fa155914c00f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inferring with RITS:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f278f0742e34d26a54a05950cde1afc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inferring with RITS:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d1d009bfb574bb09dc69157452a05cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inferring with RITS:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e42c12e3e1c14bb4aded6c1cfd64d62d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inferring with RITS:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0421762942d949359f93036c573781d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inferring with RITS:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10it [05:07, 30.74s/it]\n",
      "/Users/jasmina/miniforge3/envs/glove/lib/python3.11/site-packages/risk_policy_distillation/pipeline/pipeline.py:143: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  cds[\"Zipped\"] = cds.apply(\n",
      "/Users/jasmina/miniforge3/envs/glove/lib/python3.11/site-packages/risk_policy_distillation/pipeline/pipeline.py:143: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  cds[\"Zipped\"] = cds.apply(\n",
      "Could not cache non-existence of file. Will ignore error and continue. Error: [Errno 13] Permission denied: '/Users/jasmina/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/.no_exist/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/adapter_config.json'\n",
      "Could not cache non-existence of file. Will ignore error and continue. Error: [Errno 13] Permission denied: '/Users/jasmina/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/.no_exist/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/adapter_config.json'\n",
      "Could not cache non-existence of file. Will ignore error and continue. Error: [Errno 13] Permission denied: '/Users/jasmina/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/.no_exist/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/adapter_config.json'\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "346159dc74d4426c92408c70635a6e09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inferring with RITS:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RITS = True Model = llama-3.1-8b-instruct\n",
      "[NLIExtractor] Using LLM on RITS: llama-3.1-8b-instruct\n",
      "[NLIExtractor] Prompt version: v1\n",
      "[NLIExtractor] Prompts created: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NLI: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 48395.82prompts/s]\n",
      "Could not cache non-existence of file. Will ignore error and continue. Error: [Errno 13] Permission denied: '/Users/jasmina/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/.no_exist/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/adapter_config.json'\n",
      "Could not cache non-existence of file. Will ignore error and continue. Error: [Errno 13] Permission denied: '/Users/jasmina/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/.no_exist/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/adapter_config.json'\n",
      "Could not cache non-existence of file. Will ignore error and continue. Error: [Errno 13] Permission denied: '/Users/jasmina/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/.no_exist/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/adapter_config.json'\n",
      "Could not cache non-existence of file. Will ignore error and continue. Error: [Errno 13] Permission denied: '/Users/jasmina/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/.no_exist/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/adapter_config.json'\n",
      "Could not cache non-existence of file. Will ignore error and continue. Error: [Errno 13] Permission denied: '/Users/jasmina/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/.no_exist/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/adapter_config.json'\n",
      "Could not cache non-existence of file. Will ignore error and continue. Error: [Errno 13] Permission denied: '/Users/jasmina/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/.no_exist/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/adapter_config.json'\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6f29a5ff3fb4105bca0bcc2b1a7357f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inferring with RITS:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RITS = True Model = llama-3.1-8b-instruct\n",
      "[NLIExtractor] Using LLM on RITS: llama-3.1-8b-instruct\n",
      "[NLIExtractor] Prompt version: v1\n",
      "[NLIExtractor] Prompts created: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NLI: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 20815.40prompts/s]\n",
      "Could not cache non-existence of file. Will ignore error and continue. Error: [Errno 13] Permission denied: '/Users/jasmina/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/.no_exist/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/adapter_config.json'\n",
      "Could not cache non-existence of file. Will ignore error and continue. Error: [Errno 13] Permission denied: '/Users/jasmina/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/.no_exist/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/adapter_config.json'\n",
      "Could not cache non-existence of file. Will ignore error and continue. Error: [Errno 13] Permission denied: '/Users/jasmina/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/.no_exist/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/adapter_config.json'\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "703d9c34740c4096b2c322e46a1b6933",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inferring with RITS:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RITS = True Model = llama-3.1-8b-instruct\n",
      "[NLIExtractor] Using LLM on RITS: llama-3.1-8b-instruct\n",
      "[NLIExtractor] Prompt version: v1\n",
      "[NLIExtractor] Prompts created: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NLI: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 31300.78prompts/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53cadddf591b4d9091601852fb9981c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inferring with RITS:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RITS = True Model = llama-3.1-8b-instruct\n",
      "[NLIExtractor] Using LLM on RITS: llama-3.1-8b-instruct\n",
      "[NLIExtractor] Prompt version: v1\n",
      "[NLIExtractor] Prompts created: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NLI: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 19972.88prompts/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21113e47ed304a4ca6534ca226ebdaac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inferring with RITS:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RITS = True Model = llama-3.1-8b-instruct\n",
      "[NLIExtractor] Using LLM on RITS: llama-3.1-8b-instruct\n",
      "[NLIExtractor] Prompt version: v1\n",
      "[NLIExtractor] Prompts created: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NLI: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 58661.59prompts/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95c19019a2b840c19355e5ed1e114f83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inferring with RITS:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RITS = True Model = llama-3.1-8b-instruct\n",
      "[NLIExtractor] Using LLM on RITS: llama-3.1-8b-instruct\n",
      "[NLIExtractor] Prompt version: v1\n",
      "[NLIExtractor] Prompts created: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NLI: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 29433.71prompts/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b03a97dc3dbe43cd9b0d8838f76d7dc8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inferring with RITS:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RITS = True Model = llama-3.1-8b-instruct\n",
      "[NLIExtractor] Using LLM on RITS: llama-3.1-8b-instruct\n",
      "[NLIExtractor] Prompt version: v1\n",
      "[NLIExtractor] Prompts created: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NLI: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 39945.75prompts/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e79a910550064a6596e810c8045bca68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inferring with RITS:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RITS = True Model = llama-3.1-8b-instruct\n",
      "[NLIExtractor] Using LLM on RITS: llama-3.1-8b-instruct\n",
      "[NLIExtractor] Prompt version: v1\n",
      "[NLIExtractor] Prompts created: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NLI: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 22250.95prompts/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69cc45a3e13540c988865ae720175be7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inferring with RITS:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RITS = True Model = llama-3.1-8b-instruct\n",
      "[NLIExtractor] Using LLM on RITS: llama-3.1-8b-instruct\n",
      "[NLIExtractor] Prompt version: v1\n",
      "[NLIExtractor] Prompts created: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NLI: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 21788.59prompts/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a2184f72ddb47f6857cae065219e691",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inferring with RITS:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RITS = True Model = llama-3.1-8b-instruct\n",
      "[NLIExtractor] Using LLM on RITS: llama-3.1-8b-instruct\n",
      "[NLIExtractor] Prompt version: v1\n",
      "[NLIExtractor] Prompts created: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NLI: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 19972.88prompts/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "326a47f830434f5a9a0a760984af8359",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inferring with RITS:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RITS = True Model = llama-3.1-8b-instruct\n",
      "[NLIExtractor] Using LLM on RITS: llama-3.1-8b-instruct\n",
      "[NLIExtractor] Prompt version: v1\n",
      "[NLIExtractor] Prompts created: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NLI: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 21509.25prompts/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "836b992ee9a044b29fe30017df3abfd7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inferring with RITS:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RITS = True Model = llama-3.1-8b-instruct\n",
      "[NLIExtractor] Using LLM on RITS: llama-3.1-8b-instruct\n",
      "[NLIExtractor] Prompt version: v1\n",
      "[NLIExtractor] Prompts created: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NLI: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 16416.06prompts/s]\n",
      "Could not cache non-existence of file. Will ignore error and continue. Error: [Errno 13] Permission denied: '/Users/jasmina/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/.no_exist/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/adapter_config.json'\n",
      "Could not cache non-existence of file. Will ignore error and continue. Error: [Errno 13] Permission denied: '/Users/jasmina/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/.no_exist/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/adapter_config.json'\n",
      "Could not cache non-existence of file. Will ignore error and continue. Error: [Errno 13] Permission denied: '/Users/jasmina/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/.no_exist/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/adapter_config.json'\n",
      "Could not cache non-existence of file. Will ignore error and continue. Error: [Errno 13] Permission denied: '/Users/jasmina/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/.no_exist/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/adapter_config.json'\n",
      "Could not cache non-existence of file. Will ignore error and continue. Error: [Errno 13] Permission denied: '/Users/jasmina/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/.no_exist/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/adapter_config.json'\n",
      "Could not cache non-existence of file. Will ignore error and continue. Error: [Errno 13] Permission denied: '/Users/jasmina/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/.no_exist/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/adapter_config.json'\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a39c1bfd4abd4eef81925a4f9eae3e30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inferring with RITS:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RITS = True Model = llama-3.1-8b-instruct\n",
      "[NLIExtractor] Using LLM on RITS: llama-3.1-8b-instruct\n",
      "[NLIExtractor] Prompt version: v1\n",
      "[NLIExtractor] Prompts created: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NLI: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 64527.75prompts/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3a4a53dbae046118b8b54bdad3d75b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inferring with RITS:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RITS = True Model = llama-3.1-8b-instruct\n",
      "[NLIExtractor] Using LLM on RITS: llama-3.1-8b-instruct\n",
      "[NLIExtractor] Prompt version: v1\n",
      "[NLIExtractor] Prompts created: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NLI: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 16946.68prompts/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ceb8f8cb45046caa2cf92e0eebde054",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inferring with RITS:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RITS = True Model = llama-3.1-8b-instruct\n",
      "[NLIExtractor] Using LLM on RITS: llama-3.1-8b-instruct\n",
      "[NLIExtractor] Prompt version: v1\n",
      "[NLIExtractor] Prompts created: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NLI: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 33288.13prompts/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de037e07df484e62a4a8df7a328afe16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inferring with RITS:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RITS = True Model = llama-3.1-8b-instruct\n",
      "[NLIExtractor] Using LLM on RITS: llama-3.1-8b-instruct\n",
      "[NLIExtractor] Prompt version: v1\n",
      "[NLIExtractor] Prompts created: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NLI: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 17367.72prompts/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "905de65fb3ab4cf3b800eef1fbff8983",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inferring with RITS:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RITS = True Model = llama-3.1-8b-instruct\n",
      "[NLIExtractor] Using LLM on RITS: llama-3.1-8b-instruct\n",
      "[NLIExtractor] Prompt version: v1\n",
      "[NLIExtractor] Prompts created: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NLI: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 8656.97prompts/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43875cccf46b47d78c58c853152186fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inferring with RITS:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RITS = True Model = llama-3.1-8b-instruct\n",
      "[NLIExtractor] Using LLM on RITS: llama-3.1-8b-instruct\n",
      "[NLIExtractor] Prompt version: v1\n",
      "[NLIExtractor] Prompts created: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NLI: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 20213.51prompts/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2575e21053ab4b298f1a7f2f06c0567e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inferring with RITS:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RITS = True Model = llama-3.1-8b-instruct\n",
      "[NLIExtractor] Using LLM on RITS: llama-3.1-8b-instruct\n",
      "[NLIExtractor] Prompt version: v1\n",
      "[NLIExtractor] Prompts created: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NLI: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 12710.01prompts/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64322eb5e37240e7b3fe1060e24fcf5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inferring with RITS:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RITS = True Model = llama-3.1-8b-instruct\n",
      "[NLIExtractor] Using LLM on RITS: llama-3.1-8b-instruct\n",
      "[NLIExtractor] Prompt version: v1\n",
      "[NLIExtractor] Prompts created: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NLI: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 16008.79prompts/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "995dc61e8c674f7b859ea6ce1f0dd42b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inferring with RITS:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RITS = True Model = llama-3.1-8b-instruct\n",
      "[NLIExtractor] Using LLM on RITS: llama-3.1-8b-instruct\n",
      "[NLIExtractor] Prompt version: v1\n",
      "[NLIExtractor] Prompts created: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NLI: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 18850.80prompts/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "671e87e3ef76436c91443acd614c06c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inferring with RITS:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RITS = True Model = llama-3.1-8b-instruct\n",
      "[NLIExtractor] Using LLM on RITS: llama-3.1-8b-instruct\n",
      "[NLIExtractor] Prompt version: v1\n",
      "[NLIExtractor] Prompts created: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NLI: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 15391.94prompts/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77a81c81ce7e4633b9019ca4de23b9e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inferring with RITS:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RITS = True Model = llama-3.1-8b-instruct\n",
      "[NLIExtractor] Using LLM on RITS: llama-3.1-8b-instruct\n",
      "[NLIExtractor] Prompt version: v1\n",
      "[NLIExtractor] Prompts created: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NLI: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 10407.70prompts/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41012095a8b54f6ab5dad73a236d0c52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inferring with RITS:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RITS = True Model = llama-3.1-8b-instruct\n",
      "[NLIExtractor] Using LLM on RITS: llama-3.1-8b-instruct\n",
      "[NLIExtractor] Prompt version: v1\n",
      "[NLIExtractor] Prompts created: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NLI: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 14193.92prompts/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d75747dae8ef499786e7fcfa5c7413af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inferring with RITS:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RITS = True Model = llama-3.1-8b-instruct\n",
      "[NLIExtractor] Using LLM on RITS: llama-3.1-8b-instruct\n",
      "[NLIExtractor] Prompt version: v1\n",
      "[NLIExtractor] Prompts created: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NLI: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 28926.23prompts/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf84de15616c4398a0517dc2f80a9d9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inferring with RITS:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RITS = True Model = llama-3.1-8b-instruct\n",
      "[NLIExtractor] Using LLM on RITS: llama-3.1-8b-instruct\n",
      "[NLIExtractor] Prompt version: v1\n",
      "[NLIExtractor] Prompts created: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NLI: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 6973.07prompts/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80bf5028cf204236845b8c31f569ec42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inferring with RITS:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RITS = True Model = llama-3.1-8b-instruct\n",
      "[NLIExtractor] Using LLM on RITS: llama-3.1-8b-instruct\n",
      "[NLIExtractor] Prompt version: v1\n",
      "[NLIExtractor] Prompts created: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NLI: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 30393.51prompts/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57aa8e44ca7c43f2903f5f5b6afa38bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inferring with RITS:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RITS = True Model = llama-3.1-8b-instruct\n",
      "[NLIExtractor] Using LLM on RITS: llama-3.1-8b-instruct\n",
      "[NLIExtractor] Prompt version: v1\n",
      "[NLIExtractor] Prompts created: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NLI: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 13981.01prompts/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e174e96635184cbb907659c087e5b51e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inferring with RITS:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RITS = True Model = llama-3.1-8b-instruct\n",
      "[NLIExtractor] Using LLM on RITS: llama-3.1-8b-instruct\n",
      "[NLIExtractor] Prompt version: v1\n",
      "[NLIExtractor] Prompts created: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NLI: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 14952.96prompts/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2ef43fbb4d041149f553c9c7e4c7819",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inferring with RITS:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RITS = True Model = llama-3.1-8b-instruct\n",
      "[NLIExtractor] Using LLM on RITS: llama-3.1-8b-instruct\n",
      "[NLIExtractor] Prompt version: v1\n",
      "[NLIExtractor] Prompts created: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NLI: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 7090.96prompts/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d08511652fb44579ba0c4f0d331a64d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inferring with RITS:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RITS = True Model = llama-3.1-8b-instruct\n",
      "[NLIExtractor] Using LLM on RITS: llama-3.1-8b-instruct\n",
      "[NLIExtractor] Prompt version: v1\n",
      "[NLIExtractor] Prompts created: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NLI: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 12826.62prompts/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de115d873ccd4cac8db2acfed0b1b2f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inferring with RITS:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RITS = True Model = llama-3.1-8b-instruct\n",
      "[NLIExtractor] Using LLM on RITS: llama-3.1-8b-instruct\n",
      "[NLIExtractor] Prompt version: v1\n",
      "[NLIExtractor] Prompts created: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NLI: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 14193.92prompts/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6bd37e9db6d49069ad41cd819d0ef1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inferring with RITS:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RITS = True Model = llama-3.1-8b-instruct\n",
      "[NLIExtractor] Using LLM on RITS: llama-3.1-8b-instruct\n",
      "[NLIExtractor] Prompt version: v1\n",
      "[NLIExtractor] Prompts created: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NLI: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 16225.55prompts/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6dc6b7108f5c4efe836616e7410d8683",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inferring with RITS:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RITS = True Model = llama-3.1-8b-instruct\n",
      "[NLIExtractor] Using LLM on RITS: llama-3.1-8b-instruct\n",
      "[NLIExtractor] Prompt version: v1\n",
      "[NLIExtractor] Prompts created: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NLI: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 16810.84prompts/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5e0662e94024a12889ffc0634ee79de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inferring with RITS:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RITS = True Model = llama-3.1-8b-instruct\n",
      "[NLIExtractor] Using LLM on RITS: llama-3.1-8b-instruct\n",
      "[NLIExtractor] Prompt version: v1\n",
      "[NLIExtractor] Prompts created: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NLI: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 13797.05prompts/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44eabd7db11d4a63b95bcfcc1e5f7d59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inferring with RITS:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RITS = True Model = llama-3.1-8b-instruct\n",
      "[NLIExtractor] Using LLM on RITS: llama-3.1-8b-instruct\n",
      "[NLIExtractor] Prompt version: v1\n",
      "[NLIExtractor] Prompts created: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NLI: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 16980.99prompts/s]\n",
      "Could not cache non-existence of file. Will ignore error and continue. Error: [Errno 13] Permission denied: '/Users/jasmina/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/.no_exist/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/adapter_config.json'\n",
      "Could not cache non-existence of file. Will ignore error and continue. Error: [Errno 13] Permission denied: '/Users/jasmina/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/.no_exist/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/adapter_config.json'\n",
      "Could not cache non-existence of file. Will ignore error and continue. Error: [Errno 13] Permission denied: '/Users/jasmina/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/.no_exist/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/adapter_config.json'\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63745c5d31ea4532ab8f630ec229e339",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inferring with RITS:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RITS = True Model = llama-3.1-8b-instruct\n",
      "[NLIExtractor] Using LLM on RITS: llama-3.1-8b-instruct\n",
      "[NLIExtractor] Prompt version: v1\n",
      "[NLIExtractor] Prompts created: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NLI: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 22671.91prompts/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32d052e55c4843e6979bb56864c33002",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inferring with RITS:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RITS = True Model = llama-3.1-8b-instruct\n",
      "[NLIExtractor] Using LLM on RITS: llama-3.1-8b-instruct\n",
      "[NLIExtractor] Prompt version: v1\n",
      "[NLIExtractor] Prompts created: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NLI: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 6605.20prompts/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a84b20b0aff640da9df376fc6790ebfe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inferring with RITS:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RITS = True Model = llama-3.1-8b-instruct\n",
      "[NLIExtractor] Using LLM on RITS: llama-3.1-8b-instruct\n",
      "[NLIExtractor] Prompt version: v1\n",
      "[NLIExtractor] Prompts created: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NLI: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 17549.39prompts/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2daaaf6b09b4c45b25e7f1f2e8d1820",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inferring with RITS:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RITS = True Model = llama-3.1-8b-instruct\n",
      "[NLIExtractor] Using LLM on RITS: llama-3.1-8b-instruct\n",
      "[NLIExtractor] Prompt version: v1\n",
      "[NLIExtractor] Prompts created: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NLI: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 15169.27prompts/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bcbef40456dc49b0a878ea4dc4173e37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inferring with RITS:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RITS = True Model = llama-3.1-8b-instruct\n",
      "[NLIExtractor] Using LLM on RITS: llama-3.1-8b-instruct\n",
      "[NLIExtractor] Prompt version: v1\n",
      "[NLIExtractor] Prompts created: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NLI: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 8224.13prompts/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92e0d51bb5c34bc7a303c88fbaceedeb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inferring with RITS:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RITS = True Model = llama-3.1-8b-instruct\n",
      "[NLIExtractor] Using LLM on RITS: llama-3.1-8b-instruct\n",
      "[NLIExtractor] Prompt version: v1\n",
      "[NLIExtractor] Prompts created: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NLI: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 20164.92prompts/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1744c38dda574978998f6f6735705d6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inferring with RITS:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RITS = True Model = llama-3.1-8b-instruct\n",
      "[NLIExtractor] Using LLM on RITS: llama-3.1-8b-instruct\n",
      "[NLIExtractor] Prompt version: v1\n",
      "[NLIExtractor] Prompts created: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NLI: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 28630.06prompts/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fca31d44c0c48549c88a43c88a03be7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inferring with RITS:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RITS = True Model = llama-3.1-8b-instruct\n",
      "[NLIExtractor] Using LLM on RITS: llama-3.1-8b-instruct\n",
      "[NLIExtractor] Prompt version: v1\n",
      "[NLIExtractor] Prompts created: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NLI: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 19065.02prompts/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5667cc38c36c46fc9dd0575a64860beb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inferring with RITS:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RITS = True Model = llama-3.1-8b-instruct\n",
      "[NLIExtractor] Using LLM on RITS: llama-3.1-8b-instruct\n",
      "[NLIExtractor] Prompt version: v1\n",
      "[NLIExtractor] Prompts created: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NLI: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 15887.52prompts/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44194269bc2d408ca3dda818c086c166",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inferring with RITS:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RITS = True Model = llama-3.1-8b-instruct\n",
      "[NLIExtractor] Using LLM on RITS: llama-3.1-8b-instruct\n",
      "[NLIExtractor] Prompt version: v1\n",
      "[NLIExtractor] Prompts created: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NLI: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 20213.51prompts/s]\n",
      "Could not cache non-existence of file. Will ignore error and continue. Error: [Errno 13] Permission denied: '/Users/jasmina/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/.no_exist/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/adapter_config.json'\n",
      "Could not cache non-existence of file. Will ignore error and continue. Error: [Errno 13] Permission denied: '/Users/jasmina/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/.no_exist/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/adapter_config.json'\n",
      "Could not cache non-existence of file. Will ignore error and continue. Error: [Errno 13] Permission denied: '/Users/jasmina/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/.no_exist/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/adapter_config.json'\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e429ff4f8dc94d508176897116c5de86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inferring with RITS:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RITS = True Model = llama-3.1-8b-instruct\n",
      "[NLIExtractor] Using LLM on RITS: llama-3.1-8b-instruct\n",
      "[NLIExtractor] Prompt version: v1\n",
      "[NLIExtractor] Prompts created: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NLI: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 38597.89prompts/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14dc821816e74f449bae360829e3003b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inferring with RITS:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RITS = True Model = llama-3.1-8b-instruct\n",
      "[NLIExtractor] Using LLM on RITS: llama-3.1-8b-instruct\n",
      "[NLIExtractor] Prompt version: v1\n",
      "[NLIExtractor] Prompts created: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NLI: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 16131.94prompts/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed467d8cad524cea8c77fa8f1a7ebed4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inferring with RITS:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RITS = True Model = llama-3.1-8b-instruct\n",
      "[NLIExtractor] Using LLM on RITS: llama-3.1-8b-instruct\n",
      "[NLIExtractor] Prompt version: v1\n",
      "[NLIExtractor] Prompts created: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NLI: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 27533.72prompts/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a7dcaf3e6df4ed5bdc53dfbf0441410",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inferring with RITS:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RITS = True Model = llama-3.1-8b-instruct\n",
      "[NLIExtractor] Using LLM on RITS: llama-3.1-8b-instruct\n",
      "[NLIExtractor] Prompt version: v1\n",
      "[NLIExtractor] Prompts created: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NLI: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 43389.35prompts/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "875e6016be464b60bb8299c5d1cae0f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inferring with RITS:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RITS = True Model = llama-3.1-8b-instruct\n",
      "[NLIExtractor] Using LLM on RITS: llama-3.1-8b-instruct\n",
      "[NLIExtractor] Prompt version: v1\n",
      "[NLIExtractor] Prompts created: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NLI: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 17848.10prompts/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96a8c844546b49e4b7cf807f96f76305",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inferring with RITS:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RITS = True Model = llama-3.1-8b-instruct\n",
      "[NLIExtractor] Using LLM on RITS: llama-3.1-8b-instruct\n",
      "[NLIExtractor] Prompt version: v1\n",
      "[NLIExtractor] Prompts created: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NLI: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 25627.11prompts/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a53156195814360a3b92b59e0df9155",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inferring with RITS:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RITS = True Model = llama-3.1-8b-instruct\n",
      "[NLIExtractor] Using LLM on RITS: llama-3.1-8b-instruct\n",
      "[NLIExtractor] Prompt version: v1\n",
      "[NLIExtractor] Prompts created: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NLI: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 28276.21prompts/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08e61c03e227499ba98cb2c86afdb69c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inferring with RITS:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RITS = True Model = llama-3.1-8b-instruct\n",
      "[NLIExtractor] Using LLM on RITS: llama-3.1-8b-instruct\n",
      "[NLIExtractor] Prompt version: v1\n",
      "[NLIExtractor] Prompts created: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NLI: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 22878.02prompts/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49561497cbdc4b7c87c5450e87273dba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inferring with RITS:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RITS = True Model = llama-3.1-8b-instruct\n",
      "[NLIExtractor] Using LLM on RITS: llama-3.1-8b-instruct\n",
      "[NLIExtractor] Prompt version: v1\n",
      "[NLIExtractor] Prompts created: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NLI: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 31300.78prompts/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f84b917e8ac14c868620f720a2f8c74d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inferring with RITS:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RITS = True Model = llama-3.1-8b-instruct\n",
      "[NLIExtractor] Using LLM on RITS: llama-3.1-8b-instruct\n",
      "[NLIExtractor] Prompt version: v1\n",
      "[NLIExtractor] Prompts created: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NLI: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 57719.78prompts/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85611c8d410e4918a2e4cb1552b80e17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inferring with RITS:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RITS = True Model = llama-3.1-8b-instruct\n",
      "[NLIExtractor] Using LLM on RITS: llama-3.1-8b-instruct\n",
      "[NLIExtractor] Prompt version: v1\n",
      "[NLIExtractor] Prompts created: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NLI: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 25837.60prompts/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4fbbc1368082452c90820194e408b507",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inferring with RITS:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RITS = True Model = llama-3.1-8b-instruct\n",
      "[NLIExtractor] Using LLM on RITS: llama-3.1-8b-instruct\n",
      "[NLIExtractor] Prompt version: v1\n",
      "[NLIExtractor] Prompts created: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NLI: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 24769.51prompts/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d1aee1784a54900ac79ab087158d76c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inferring with RITS:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RITS = True Model = llama-3.1-8b-instruct\n",
      "[NLIExtractor] Using LLM on RITS: llama-3.1-8b-instruct\n",
      "[NLIExtractor] Prompt version: v1\n",
      "[NLIExtractor] Prompts created: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NLI: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 31615.36prompts/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f01320836ba4809a27539bb8be4fdae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inferring with RITS:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RITS = True Model = llama-3.1-8b-instruct\n",
      "[NLIExtractor] Using LLM on RITS: llama-3.1-8b-instruct\n",
      "[NLIExtractor] Prompt version: v1\n",
      "[NLIExtractor] Prompts created: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NLI: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 24966.10prompts/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58b6e6890501439794b1f83f89376785",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inferring with RITS:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RITS = True Model = llama-3.1-8b-instruct\n",
      "[NLIExtractor] Using LLM on RITS: llama-3.1-8b-instruct\n",
      "[NLIExtractor] Prompt version: v1\n",
      "[NLIExtractor] Prompts created: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NLI: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 24576.00prompts/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4545f1853d41494395ea71c644debce4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inferring with RITS:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RITS = True Model = llama-3.1-8b-instruct\n",
      "[NLIExtractor] Using LLM on RITS: llama-3.1-8b-instruct\n",
      "[NLIExtractor] Prompt version: v1\n",
      "[NLIExtractor] Prompts created: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NLI: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 37560.93prompts/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "478187251c374859b241e2fb30d6953b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inferring with RITS:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RITS = True Model = llama-3.1-8b-instruct\n",
      "[NLIExtractor] Using LLM on RITS: llama-3.1-8b-instruct\n",
      "[NLIExtractor] Prompt version: v1\n",
      "[NLIExtractor] Prompts created: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NLI: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 25890.77prompts/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c19c139a5d444320ad60a6b925e4e936",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inferring with RITS:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RITS = True Model = llama-3.1-8b-instruct\n",
      "[NLIExtractor] Using LLM on RITS: llama-3.1-8b-instruct\n",
      "[NLIExtractor] Prompt version: v1\n",
      "[NLIExtractor] Prompts created: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NLI: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 21583.04prompts/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2e22a85dd204ed6a8311e47b79dd7db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inferring with RITS:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RITS = True Model = llama-3.1-8b-instruct\n",
      "[NLIExtractor] Using LLM on RITS: llama-3.1-8b-instruct\n",
      "[NLIExtractor] Prompt version: v1\n",
      "[NLIExtractor] Prompts created: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NLI: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 24013.19prompts/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ab2f429a742451a8ff1e5d5c04d0397",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inferring with RITS:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RITS = True Model = llama-3.1-8b-instruct\n",
      "[NLIExtractor] Using LLM on RITS: llama-3.1-8b-instruct\n",
      "[NLIExtractor] Prompt version: v1\n",
      "[NLIExtractor] Prompts created: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NLI: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 27235.74prompts/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e62616afb014b0482e362290d15a381",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inferring with RITS:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RITS = True Model = llama-3.1-8b-instruct\n",
      "[NLIExtractor] Using LLM on RITS: llama-3.1-8b-instruct\n",
      "[NLIExtractor] Prompt version: v1\n",
      "[NLIExtractor] Prompts created: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NLI: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 18850.80prompts/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "540c232c14de47358aa1a76beaec98b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inferring with RITS:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RITS = True Model = llama-3.1-8b-instruct\n",
      "[NLIExtractor] Using LLM on RITS: llama-3.1-8b-instruct\n",
      "[NLIExtractor] Prompt version: v1\n",
      "[NLIExtractor] Prompts created: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NLI: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 21076.90prompts/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68df4c4f83f54399b45c1a8fc83882fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inferring with RITS:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RITS = True Model = llama-3.1-8b-instruct\n",
      "[NLIExtractor] Using LLM on RITS: llama-3.1-8b-instruct\n",
      "[NLIExtractor] Prompt version: v1\n",
      "[NLIExtractor] Prompts created: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NLI: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 17734.90prompts/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4d26086c99b43c0b98b39cc499ce70b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inferring with RITS:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RITS = True Model = llama-3.1-8b-instruct\n",
      "[NLIExtractor] Using LLM on RITS: llama-3.1-8b-instruct\n",
      "[NLIExtractor] Prompt version: v1\n",
      "[NLIExtractor] Prompts created: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NLI: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 24105.20prompts/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95531047314440bca802dd4c3328c240",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inferring with RITS:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RITS = True Model = llama-3.1-8b-instruct\n",
      "[NLIExtractor] Using LLM on RITS: llama-3.1-8b-instruct\n",
      "[NLIExtractor] Prompt version: v1\n",
      "[NLIExtractor] Prompts created: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NLI: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 18157.16prompts/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd82872b8ab34fd4835dce51c750bd2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inferring with RITS:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RITS = True Model = llama-3.1-8b-instruct\n",
      "[NLIExtractor] Using LLM on RITS: llama-3.1-8b-instruct\n",
      "[NLIExtractor] Prompt version: v1\n",
      "[NLIExtractor] Prompts created: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NLI: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 20610.83prompts/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebdd699240e3497c95fa969e002bc1bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inferring with RITS:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RITS = True Model = llama-3.1-8b-instruct\n",
      "[NLIExtractor] Using LLM on RITS: llama-3.1-8b-instruct\n",
      "[NLIExtractor] Prompt version: v1\n",
      "[NLIExtractor] Prompts created: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NLI: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 18682.87prompts/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da78d7b59ebb477b87531e148040b520",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inferring with RITS:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RITS = True Model = llama-3.1-8b-instruct\n",
      "[NLIExtractor] Using LLM on RITS: llama-3.1-8b-instruct\n",
      "[NLIExtractor] Prompt version: v1\n",
      "[NLIExtractor] Prompts created: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NLI: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 14004.35prompts/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06f08fdca3df471185417694ec7b5f88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inferring with RITS:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RITS = True Model = llama-3.1-8b-instruct\n",
      "[NLIExtractor] Using LLM on RITS: llama-3.1-8b-instruct\n",
      "[NLIExtractor] Prompt version: v1\n",
      "[NLIExtractor] Prompts created: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NLI: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 15505.74prompts/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa1690fca32f4f40a1c6b6e629aef2c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inferring with RITS:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RITS = True Model = llama-3.1-8b-instruct\n",
      "[NLIExtractor] Using LLM on RITS: llama-3.1-8b-instruct\n",
      "[NLIExtractor] Prompt version: v1\n",
      "[NLIExtractor] Prompts created: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NLI: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 18850.80prompts/s]\n",
      "Could not cache non-existence of file. Will ignore error and continue. Error: [Errno 13] Permission denied: '/Users/jasmina/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/.no_exist/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/adapter_config.json'\n",
      "Could not cache non-existence of file. Will ignore error and continue. Error: [Errno 13] Permission denied: '/Users/jasmina/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/.no_exist/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/adapter_config.json'\n",
      "Could not cache non-existence of file. Will ignore error and continue. Error: [Errno 13] Permission denied: '/Users/jasmina/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/.no_exist/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/adapter_config.json'\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e6a7e716ee74f56af5acca1a602517b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inferring with RITS:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RITS = True Model = llama-3.1-8b-instruct\n",
      "[NLIExtractor] Using LLM on RITS: llama-3.1-8b-instruct\n",
      "[NLIExtractor] Prompt version: v1\n",
      "[NLIExtractor] Prompts created: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NLI: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00, 29433.71prompts/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6af85a670b74b669080532472ba7c10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inferring with RITS:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RITS = True Model = llama-3.1-8b-instruct\n",
      "[NLIExtractor] Using LLM on RITS: llama-3.1-8b-instruct\n",
      "[NLIExtractor] Prompt version: v1\n",
      "[NLIExtractor] Prompts created: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NLI: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00, 39290.90prompts/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1532c4a2d71d4684a9be5d46cee5ed88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inferring with RITS:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RITS = True Model = llama-3.1-8b-instruct\n",
      "[NLIExtractor] Using LLM on RITS: llama-3.1-8b-instruct\n",
      "[NLIExtractor] Prompt version: v1\n",
      "[NLIExtractor] Prompts created: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NLI: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00, 78398.21prompts/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a4f0be330d348f59dedccf875bcdba6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inferring with RITS:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RITS = True Model = llama-3.1-8b-instruct\n",
      "[NLIExtractor] Using LLM on RITS: llama-3.1-8b-instruct\n",
      "[NLIExtractor] Prompt version: v1\n",
      "[NLIExtractor] Prompts created: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NLI: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00, 37035.80prompts/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71bc321e0b5b42fe8a6578fd474897ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inferring with RITS:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RITS = True Model = llama-3.1-8b-instruct\n",
      "[NLIExtractor] Using LLM on RITS: llama-3.1-8b-instruct\n",
      "[NLIExtractor] Prompt version: v1\n",
      "[NLIExtractor] Prompts created: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NLI: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00, 33288.13prompts/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f9c8c67654c4c2f8195c95de6fb3aff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inferring with RITS:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RITS = True Model = llama-3.1-8b-instruct\n",
      "[NLIExtractor] Using LLM on RITS: llama-3.1-8b-instruct\n",
      "[NLIExtractor] Prompt version: v1\n",
      "[NLIExtractor] Prompts created: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NLI: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00, 31956.60prompts/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c685fbb952bf4c8993cebd245f02d0b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inferring with RITS:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RITS = True Model = llama-3.1-8b-instruct\n",
      "[NLIExtractor] Using LLM on RITS: llama-3.1-8b-instruct\n",
      "[NLIExtractor] Prompt version: v1\n",
      "[NLIExtractor] Prompts created: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NLI: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00, 41630.81prompts/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69c00e1162f64680923ad67436ec273b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inferring with RITS:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RITS = True Model = llama-3.1-8b-instruct\n",
      "[NLIExtractor] Using LLM on RITS: llama-3.1-8b-instruct\n",
      "[NLIExtractor] Prompt version: v1\n",
      "[NLIExtractor] Prompts created: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NLI: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00, 54120.05prompts/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74ebae26ef564fa685183962aa9863e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inferring with RITS:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RITS = True Model = llama-3.1-8b-instruct\n",
      "[NLIExtractor] Using LLM on RITS: llama-3.1-8b-instruct\n",
      "[NLIExtractor] Prompt version: v1\n",
      "[NLIExtractor] Prompts created: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NLI: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00, 31242.49prompts/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78b7198b6d93424fae4d05f51e41145b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inferring with RITS:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RITS = True Model = llama-3.1-8b-instruct\n",
      "[NLIExtractor] Using LLM on RITS: llama-3.1-8b-instruct\n",
      "[NLIExtractor] Prompt version: v1\n",
      "[NLIExtractor] Prompts created: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NLI: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00, 14926.35prompts/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c5a9c19999a47f29b73af8016027b28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inferring with RITS:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RITS = True Model = llama-3.1-8b-instruct\n",
      "[NLIExtractor] Using LLM on RITS: llama-3.1-8b-instruct\n",
      "[NLIExtractor] Prompt version: v1\n",
      "[NLIExtractor] Prompts created: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NLI: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 23045.63prompts/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84ffc21aec134af395a0f9afe2fb37dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inferring with RITS:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RITS = True Model = llama-3.1-8b-instruct\n",
      "[NLIExtractor] Using LLM on RITS: llama-3.1-8b-instruct\n",
      "[NLIExtractor] Prompt version: v1\n",
      "[NLIExtractor] Prompts created: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NLI: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 26772.15prompts/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2237dff8d25249eebbca9672d5f6e558",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inferring with RITS:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RITS = True Model = llama-3.1-8b-instruct\n",
      "[NLIExtractor] Using LLM on RITS: llama-3.1-8b-instruct\n",
      "[NLIExtractor] Prompt version: v1\n",
      "[NLIExtractor] Prompts created: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NLI: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 40590.04prompts/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8d332457ad942aa89e7403f4e8153ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inferring with RITS:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RITS = True Model = llama-3.1-8b-instruct\n",
      "[NLIExtractor] Using LLM on RITS: llama-3.1-8b-instruct\n",
      "[NLIExtractor] Prompt version: v1\n",
      "[NLIExtractor] Prompts created: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NLI: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 9467.95prompts/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbe7ef07871449648e4cda48481723b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inferring with RITS:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RITS = True Model = llama-3.1-8b-instruct\n",
      "[NLIExtractor] Using LLM on RITS: llama-3.1-8b-instruct\n",
      "[NLIExtractor] Prompt version: v1\n",
      "[NLIExtractor] Prompts created: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NLI: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 16299.11prompts/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "069cd1f7423d4721beda10c6e4bb0d82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inferring with RITS:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RITS = True Model = llama-3.1-8b-instruct\n",
      "[NLIExtractor] Using LLM on RITS: llama-3.1-8b-instruct\n",
      "[NLIExtractor] Prompt version: v1\n",
      "[NLIExtractor] Prompts created: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NLI: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 34473.73prompts/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f727ae33902b44ccbc4bd3350e028c9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inferring with RITS:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RITS = True Model = llama-3.1-8b-instruct\n",
      "[NLIExtractor] Using LLM on RITS: llama-3.1-8b-instruct\n",
      "[NLIExtractor] Prompt version: v1\n",
      "[NLIExtractor] Prompts created: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NLI: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 26772.15prompts/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e78ea0fd2f6422a96314514e74d4137",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inferring with RITS:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RITS = True Model = llama-3.1-8b-instruct\n",
      "[NLIExtractor] Using LLM on RITS: llama-3.1-8b-instruct\n",
      "[NLIExtractor] Prompt version: v1\n",
      "[NLIExtractor] Prompts created: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NLI: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 21147.75prompts/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22eb1052ea1e451296a068d1c406a24f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inferring with RITS:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RITS = True Model = llama-3.1-8b-instruct\n",
      "[NLIExtractor] Using LLM on RITS: llama-3.1-8b-instruct\n",
      "[NLIExtractor] Prompt version: v1\n",
      "[NLIExtractor] Prompts created: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NLI: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 22075.28prompts/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a7de8bd7e0d4e0bbdd167c73677577f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inferring with RITS:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RITS = True Model = llama-3.1-8b-instruct\n",
      "[NLIExtractor] Using LLM on RITS: llama-3.1-8b-instruct\n",
      "[NLIExtractor] Prompt version: v1\n",
      "[NLIExtractor] Prompts created: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NLI: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 29959.31prompts/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a71dd6d175c54aacb4a1175abf86365c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inferring with RITS:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RITS = True Model = llama-3.1-8b-instruct\n",
      "[NLIExtractor] Using LLM on RITS: llama-3.1-8b-instruct\n",
      "[NLIExtractor] Prompt version: v1\n",
      "[NLIExtractor] Prompts created: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NLI: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 29433.71prompts/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f2bdef83f394497ab66fdf0ec8bc3fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inferring with RITS:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RITS = True Model = llama-3.1-8b-instruct\n",
      "[NLIExtractor] Using LLM on RITS: llama-3.1-8b-instruct\n",
      "[NLIExtractor] Prompt version: v1\n",
      "[NLIExtractor] Prompts created: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NLI: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 23237.14prompts/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6609edaaea654eb082f68035a1b501eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inferring with RITS:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RITS = True Model = llama-3.1-8b-instruct\n",
      "[NLIExtractor] Using LLM on RITS: llama-3.1-8b-instruct\n",
      "[NLIExtractor] Prompt version: v1\n",
      "[NLIExtractor] Prompts created: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NLI: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 44384.17prompts/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e505ebe21f94cf8b3206ae18a5d045f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inferring with RITS:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RITS = True Model = llama-3.1-8b-instruct\n",
      "[NLIExtractor] Using LLM on RITS: llama-3.1-8b-instruct\n",
      "[NLIExtractor] Prompt version: v1\n",
      "[NLIExtractor] Prompts created: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NLI: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 34952.53prompts/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "484c84cd5e404e4f8d64a6ba74b77019",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inferring with RITS:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RITS = True Model = llama-3.1-8b-instruct\n",
      "[NLIExtractor] Using LLM on RITS: llama-3.1-8b-instruct\n",
      "[NLIExtractor] Prompt version: v1\n",
      "[NLIExtractor] Prompts created: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NLI: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 41734.37prompts/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9239b5f083e545f9a073c4a2d3b4e1f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inferring with RITS:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RITS = True Model = llama-3.1-8b-instruct\n",
      "[NLIExtractor] Using LLM on RITS: llama-3.1-8b-instruct\n",
      "[NLIExtractor] Prompt version: v1\n",
      "[NLIExtractor] Prompts created: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NLI: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 14488.10prompts/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72cc98c1ecc6400087c82df85296ec09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inferring with RITS:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RITS = True Model = llama-3.1-8b-instruct\n",
      "[NLIExtractor] Using LLM on RITS: llama-3.1-8b-instruct\n",
      "[NLIExtractor] Prompt version: v1\n",
      "[NLIExtractor] Prompts created: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NLI: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 29026.33prompts/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c29e5a2402f64568a55e303ce1dc3416",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inferring with RITS:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RITS = True Model = llama-3.1-8b-instruct\n",
      "[NLIExtractor] Using LLM on RITS: llama-3.1-8b-instruct\n",
      "[NLIExtractor] Prompt version: v1\n",
      "[NLIExtractor] Prompts created: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NLI: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 32896.50prompts/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2885b977d3f34a83b029821ea778bdd1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inferring with RITS:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RITS = True Model = llama-3.1-8b-instruct\n",
      "[NLIExtractor] Using LLM on RITS: llama-3.1-8b-instruct\n",
      "[NLIExtractor] Prompt version: v1\n",
      "[NLIExtractor] Prompts created: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NLI: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 27060.03prompts/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14980ff8e9ca47eeb68e729a7117406e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inferring with RITS:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RITS = True Model = llama-3.1-8b-instruct\n",
      "[NLIExtractor] Using LLM on RITS: llama-3.1-8b-instruct\n",
      "[NLIExtractor] Prompt version: v1\n",
      "[NLIExtractor] Prompts created: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NLI: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 17549.39prompts/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rules': [{'prediction': 0, 'if_clause': 'uses respectful language', 'despite_clauses': 'none'}, {'prediction': 1, 'if_clause': 'contains hurtful social bias', 'despite_clauses': 'none'}, {'prediction': 0, 'if_clause': 'no offensive content', 'despite_clauses': 'none'}, {'prediction': 1, 'if_clause': 'damage information provided', 'despite_clauses': 'none'}, {'prediction': 1, 'if_clause': 'explicit instructions', 'despite_clauses': 'none'}, {'prediction': 1, 'if_clause': 'weapon creation', 'despite_clauses': 'none'}, {'prediction': 1, 'if_clause': 'derogatory language', 'despite_clauses': 'none'}, {'prediction': 1, 'if_clause': 'explicit harm method', 'despite_clauses': 'none'}, {'prediction': 1, 'if_clause': 'physical harm', 'despite_clauses': 'none'}, {'prediction': 1, 'if_clause': 'explicit language', 'despite_clauses': 'none'}, {'prediction': 1, 'if_clause': 'objectifying language', 'despite_clauses': 'none'}, {'prediction': 0, 'if_clause': 'helpful advice', 'despite_clauses': 'none'}]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "risk_atlas_nexus = RiskAtlasNexus()\n",
    "\n",
    "expl = risk_atlas_nexus.generate_policy_rules(\n",
    "    guardian_config, dataset, guardian_judge, llm_component, local_expl=\"LIME\"\n",
    ")\n",
    "\n",
    "print(expl.print())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a873622e-5ce1-45d5-95df-fa8e408ab46d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
